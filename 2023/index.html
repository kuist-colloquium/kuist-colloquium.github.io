<!DOCTYPE html>
<html lang="ja">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <title>KUIST Colloquium 
2023
</title>
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
        <link rel="stylesheet" href="/style.css">
    </head>
    <body>
        <header style="background: #00205B;">
            <img src="/C-EL-W.svg" height=45 style="margin: 1em 0em 0em 1em;">
            <h1 style="color: #FFF; text-align: center; font-family: serif; font-size:3em; padding: 0.4em;">
                IST COLLOQUIUM 
2023

            </h1>
        </header>
        <section class="section">
            <div class="container">
                
<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Reducing the Need of Labelled Data in Machine Learning</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src="&#x2F;tlampert.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>Thomas Lampert</b>
        
            <span style="font-size: 1.25rem"><a href="https:&#x2F;&#x2F;sites.google.com&#x2F;site&#x2F;tomalampert">ğŸŒ</a></span>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">UniversitÃ© de Strasbourg</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
        Thomas Lampert is a Computer Science researcher in the general field of Artificial Intelligence, and more specifically in Machine Learning and Image and Time-Series Analysis in various fields of application (most recently in medical imaging and remote sensing). Dr. Lampert currently holds the Chair of Data Science and Artificial Intelligence at TÃ©lÃ©com Physique Strasbourg and ICube, University of Strasbourg.
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š2æœˆ22æ—¥ï¼ˆæœ¨ï¼‰ 
         13:15ã€œ14:45 
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶7å·é¤¨ã‚»ãƒŸãƒŠãƒ¼å®¤1 (127å·å®¤)
        </p>
        
        
        <p class="card-text">During this talk I will expose the approaches that we have developed to reduce the need for labelled data for several domains of machine learning applications. First I will discuss histopathological images and the difficulties it poses for deep learning segmentation algorithms. I will present several domain invariant approaches that we have developed to overcome these difficulties, and some interest findings that have arisen from them. Next I will discuss the difficulty of performing domain adaptation and learning domain invariant features with multi-modal imagery data (i.e. image domains with different numbers of bands), with application to remote sensing data. Finally, I will present our work on constrained clustering for time-series, a semi-supervised approach to reduce the need for labelled data using must-link and cannot-link constraints. I will extend this to propose an approach to explain the clustering result in an easily interpretable manner.</p>
        
        
            
        
    </div>
</div>
<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Is my â€œredâ€ your â€œredâ€? A structural approach on the issue of qualia</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src="&#x2F;tsuchiyanao.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>Nao (Naotsugu) Tsuchiya</b>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">Professor, School of Psychological Sciences, Turner Institute for Brain and Mental Health, Monash University, Australia</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
        Dr Tsuchiya was awarded a PhD at California Institute of Technology (Caltech) in 2006 and underwent postdoctoral training at Caltech until 2010. Receiving a PRESTO grant from Japan Science and Technology (JST) agency, Dr Tsuchiya returned to Japan in 2010. In Jan 2012, he joined the School of Psychological Sciences at Monash University as an Associate Professor. Since 2013, he is an ARC Future Fellow. His main research interest is to uncover the neuronal basis of consciousness. Specifically, he focuses on 1) the scope and limit of non-conscious processing, 2) the relationship between attention and consciousness, and 3) the neuronal correlates of consciousness by analysing the multi-channel neuronal recording obtained in animals and humans and 4) testing a theory of consciousness, in particular, integrated information theory of consciousness.
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š12æœˆ27æ—¥ï¼ˆæ°´ï¼‰ 
         13:30ã€œ14:45 
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶12å·é¤¨316å·å®¤
        </p>
        
        
        <p class="card-text">Do I experience the world in the â€œsameâ€ way as you do? How similar is my consciousness to other humans, animals, insects, octopuses, and artificial intelligence? In this talk, I will discuss this issue focusing on the content of consciousness, what-it-feels-like, or in short â€œqualiaâ€. Can we scientifically investigate if my red qualia is similar to your red qualia? Traditionally, the answer has been No. Because "qualia" are so intrinsic and purely subjective, qualia have often been considered as outside of the realm of scientific inquiry. Recently, our group has proposed a method of characterising a quale in terms of its relation to all other qualia, inspired by a mathematical theorem called â€œYoneda lemmaâ€ in the field of category theory. Based on this idea, we conducted experiments in which we asked a large number (>500) of neurotypical and colorblind subjects to report the similarity of a subset of ~5000 colour combinations in an online setting. Using the similarity structures estimated from these data, we quantified whether it is possible to â€œalignâ€ the colour qualia structures between different populations, using an unsupervised method, called â€œoptimal transportâ€ in the field of machine translation (<a href="https://psyarxiv.com/h3pqm">https://psyarxiv.com/h3pqm</a>). Our qualia structure approach is generalizable to qualia in other domains (such as similarity of evoked emotional experience of short movies), or even to structures between qualia structures. The relationship between qualia structures may eventually provide an opportunity to address questions such as, â€œWhy are colour qualia perceived as colour qualia?â€</p>
        
        
            
        
    </div>
</div>
<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Learning to Synthesize Image and Video Contents</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src="&#x2F;minghsuanyang.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>Ming-Hsuan Yang</b>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">Professor, University of California, Merced, USA</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
        Ming-Hsuan Yang is a Professor in Electrical Engineering and Computer Science at University of California, Merced, and a Research Scientist at Google. He received a Ph.D. degree in Computer Science from the University of Illinois at Urbana-Champaign in 2000. He served as a Program Chair for IEEE International Conference on Computer Vision (ICCV) in 2019 and Asian Conference on Computer Vision (ACCV) in 2014. He is as an Associate Editor-in-Chief of the IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) from 2023, co-Editor-in-Chief of Computer Vision and Image Understanding (CVIU) from 2022 to 2023, and Associate Editor of International Journal of Computer Vision (IJCV). Yang received Longuet-Higgins (Test-of-Time) Prize at CVPR in 2023, CAREER award from the National Science Foundation in 2012, and Google Faculty Award in 2009. He is a Fellow of the IEEE and ACM.
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š12æœˆ4æ—¥ï¼ˆæœˆï¼‰ 
         13:30ã€œ14:30 
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶7å·é¤¨ æƒ…å ±1è¬›ç¾©å®¤ (1éš 107)
        </p>
        
        
        <p class="card-text">In this talk, I will first review our work on learning to synthesize image and video content from image data. The underlying theme is to exploit different priors to synthesize diverse content with robust formulations. When time allows, I can also discuss other recent results on image editing, video generation, learning features, surface normal estimation, and deformable 3D reconstruction.</p>
        
        
            
        
    </div>
</div>
<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Deep Surface Meshes</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src="&#x2F;fuapascal.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>Pascal Fua</b>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">Professor, Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne, Switzerland</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
        Pascal Fua received an engineering degree from Ecole Polytechnique,Paris, in 1984 and a Ph.D. in Computer Science from the University of Orsay in 1989. He joined EPFL (Swiss Federal Institute of Technology) in 1996 where he is a Professor in the School of Computer and Communication Science and head of the Computer Vision Lab. Before that, he worked at SRI International and at INRIA Sophia-Antipolis as a Computer Scientist.<br><br>His research interests include shape modeling and motion recovery from images, analysis of microscopy images, and machine learning. He has (co)authored over 300 publications in refereed journals and conferences. He has received several ERC grants. He is an IEEE Fellow and has been an Associate Editor of IEEE journal Transactions for Pattern Analysis and Machine Intelligence. He often serves as program committee member, area chair, and program chair of major vision conferences and has cofounded three spinoff companies.
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š10æœˆ20æ—¥ï¼ˆé‡‘ï¼‰ 
         13:30ã€œ14:30 
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶7å·é¤¨ æƒ…å ±1è¬›ç¾©å®¤ (1éš 107)
        </p>
        
        
        <p class="card-text">Geometric Deep Learning has made striking progress with the advent of Deep Implicit Fields. They allow for detailed modeling of surfaces of arbitrary topology while not relying on a 3D Euclidean grid, resulting in a learnable 3D surface parameterization that is not limited in resolution. Unfortunately, they have not yet reached their full potential for applications that require an explicit surface representation in terms of vertices and facets because converting the implicit representation to such an explicit representation requires a marching-cube algorithm, whose output cannot be easily differentiated with respect to the implicit surface parameters.<br><br>In this talk, I will present our approach to overcoming this limitation and implementing convolutional neural nets that output complex 3D surface meshes while remaining fully-differentiable and end-to-end trainable. I will also present applications to single view reconstruction, physically-driven Shape optimization, and bio-medical image segmentation.</p>
        
        
            
        
    </div>
</div>
<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Human-like Motion Generation and Prediction</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src="&#x2F;ukita.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>æµ®ç”° å®—ä¼¯</b>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">è±Šç”°å·¥æ¥­å¤§å­¦ æ•™æˆ</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
        Norimichi Ukita (Member, IEEE) received the BE and ME degrees in information engineering from Okayama University, Japan, in 1996 and 1998, respectively, and the PhD degree in informatics from Kyoto University, Japan, in 2001. He is a professor with the graduate school of engineering, Toyota Technological Institute, Japan (TTI-J). After working for five years as an assistant professor at NAIST, he became an associate professor, in 2007 and moved to TTI-J, in 2016. He was a research scientist of Precursory Research for Embryonic Science and Technology, Japan Science and Technology Agency (JST), during 2002 -2006. He was a visiting research scientist at Carnegie Mellon University during 2007-2009. He currently works also at Toyota Technological Institute at Chicago (TTI-C) as an adjoint professor. His main research interests are low-level vision, object detection/tracking, and human pose estimation and action recognition.
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š9æœˆ22æ—¥ï¼ˆé‡‘ï¼‰ 
         13:30ã€œ14:30 
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶7å·é¤¨ æƒ…å ±1è¬›ç¾©å®¤ (1éš 107)
        </p>
        
        
        <p class="card-text">Generation and prediction of time-series data have a variety of real-world applications. In particular, we focus on human-like motion generation and prediction. In this presentation, (1) super-fast task-agnostic probabilistic prediction, (2) physically-constrained human motion generation from a limited number of motion samples, and (3) robot motion planning with the initial state presented by an image from a limited number of motion samples. For (1) prediction, previous methods have either of the following two problems: (i) non-probabilistic (i.e., deterministic) prediction methods cannot represent the stochasticity and diversity of possible motions and (ii) computationally slow methods that are not applicable to real-time applications. Our method is extended from normalizing flow, which enables probabilistic prediction, so that past motion transformations whose computational cost is a bottleneck are reused for real-time processing. In (2), the policy of controlling a human kinematic model is trained so that its motion gets close to real human motions in a physics simulator. For robot motion learning with a limited number of motion samples, we propose the following methods: (i) Transformer-based network in which high-resolution but efficient temporal features are extracted so that the features are spatially aligned with sample motions for representing precise robotic motions such as grasping an object and (ii) Diffusion-based network for retrieving the sample motion most probable to achieve a given task and for rectifying the retrieved motion to improve the achievability.</p>
        
        
            
        
    </div>
</div>
<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>From Videos to 4D Worlds and Beyond</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src="&#x2F;kanezawa.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>Angjoo Kanazawa</b>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">Assistant Professor, University of California, Berkeley, USA</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
        Angjoo Kanazawa is an Assistant Professor in the Department of Electrical Engineering and Computer Science at the University of California at Berkeley. Her research is at the intersection of Computer Vision, Computer Graphics, and Machine Learning, focusing on the visual perception of the dynamic 3D world behind everyday photographs and video. Previously, she was a research scientist at Google NYC, and prior to that she was a BAIR postdoc at UC Berkeley. She completed her PhD in Computer Science at the University of Maryland, College Park, where she also spent time at the Max Planck Institute for Intelligent Systems. She has been named a Rising Star in EECS and has been honored with the Google Research Scholar Award and most recently the Sloan Fellowship 2023.
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š8æœˆ25æ—¥ï¼ˆé‡‘ï¼‰ 
         13:30ã€œ14:30 
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶7å·é¤¨ æƒ…å ±1è¬›ç¾©å®¤ (1éš 107)
        </p>
        
        
        <p class="card-text">The world underlying images and videos is 3-dimensional and dynamic, with people interacting with each other, objects, and the underlying scene. Even in videos of a static scene, there is always the camera moving about in the 4D world. However, disentangling this 4D world from a video is a challenging inverse problem due to fundamental ambiguities of depth and scale. Yet, accurately recovering this information is essential for building systems that can reason about and interact with the underlying scene, and has immediate applications in visual effects and creation of immersive digital worlds. In this talk, I will discuss recent updates in 4D human perception, which includes disentangling the camera and the human motion from challenging in-the-wild videos with multiple people. Our approach takes advantage of background pixels as cues for camera motion, which when combined with motion priors and inferred ground planes can resolve scene scale and depth ambiguities up to an "anthropometric" scale. I will also talk about nerf.studio, a modular open-source framework for easily creating photorealistic 3D scenes and accelerating NeRF development. I will introduce two new works that highlight how language can be incorporated for editing and interacting with the recovered 3D scenes. These works leverage large-scale vision and language models, demonstrating the potential for multi-modal exploration and manipulation of 3D scenes</p>
        
        
            
        
    </div>
</div>
<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Visual Generation and Recognition via Object Completion</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src="&#x2F;jshi.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>Jianbo Shi</b>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">Professor, University of Pennsylvania, USA</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
        Jianbo studied Computer Science and Mathematics as an undergraduate at Cornell University where he received his B.A. in 1994. He received his Ph.D. degree in Computer Science from University of California at Berkeley in 1998, for his thesis on Normalize Cuts image segmentation algorithm. He joined The Robotics Institute at Carnegie Mellon University in 1999 as a research faculty. Since 2003, he has been with the Department of Computer & Information Science at the University of Pennsylvania. His group is developing vision algorithms for both human and image recognition. Their ultimate goal is to develop computation algorithms to understand human behavior and interaction with objects, and to do so at multiple levels of abstractions: from the basic body limb tracking, to human identification, gesture recognition, and activity inference. His group is developing a visual thinking model that allows computers to understand their surroundings and achieve higher-level cognitive abilities such as machine memory and learning.
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š8æœˆ23æ—¥ï¼ˆæ°´ï¼‰ 
         13:30ã€œ14:30 
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶7å·é¤¨æƒ…å ±2è¬›ç¾©å®¤ï¼ˆ1éš 101ï¼‰
        </p>
        
        
        <p class="card-text">Using the latest image generation tools, we study how generation and recognition models are related.  First, we explore amodal completion with Text-to-Image guidance: iteratively inpaint occluder regions for a given query object, eliminating the need for intermediate amodal mask prediction. We identify a crucial need to quantify object completeness, to prevent regenerating common co-occurring occluders and attached objects.  Our amodal completion algorithm works in tandem with the dataset creation: the inpainting algorithm creates a large set of realistic images for human experts to label. We developed a human-curated Amodal Completion of Common Objects (ACCO) dataset containing 80 common object categories in the COCO dataset.<br><br>We then study why how generation models enhance visual recognition. We propose a diverse outpainting model to synthesize and comprehend potential background interactions with an object. We utilize a pretrained inpainting model to generate labels for object-context spatial relationships, enabling the training of models to predict plausible object placement and affordance in a scene. Using the learned object placement model, we demonstrate the effectiveness of compositing objects into different compatible contexts as a data augmentation technique for object detection and instance segmentation.<br><br>Combining these two studies, we observe a nested structured encoding of object-context, similar to that of AND-Or graph, emerging from self-trained image generation models.</p>
        
        
            
        
    </div>
</div>
<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Generation Meets Reconstruction. Looking at 3D Computer Vision through the Lens of Generative AI</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src="&#x2F;furukawa.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>Yasutaka Furukawa</b>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">Associate Professor, Simon Fraser University, Canada</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
        Dr. Yasutaka Furukawa is an associate professor in the School of Computing Science at Simon Fraser University (SFU). Dr. Furukawa's group has made fundamental and practical contributions to 3D reconstruction algorithms, improved localization techniques, and computational architectural modeling. Their open-source software has been widely adopted by tech companies and used in surprising applications such as 3D printing of turtle shells and archaeological reconstruction. Dr. Furukawa received the best student paper award at ECCV 2012, the NSF CAREER Award in 2015, CS-CAN Outstanding Young CS Researcher Award 2018, Google Faculty Research Awards in 2016, 2017, and 2018, and PAMI Longuet-Higgins prize in 2020.
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š8æœˆ9æ—¥ï¼ˆæ°´ï¼‰ 
         14:00ã€œ15:00 
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶7å·é¤¨ ã‚»ãƒŸãƒŠãƒ¼å®¤2 (1éš 131)
        </p>
        
        
        <p class="card-text">Generative AI has seen a surge in popularity, with Diffusion Models (DMs) being a crucial component of many successful visual content generation techniques. Examples include DALL-E by OpenAI, Imagen by Google, and Stable Diffusion by Stability AI. While DMs are commonly known for their ability to generate content, our research group has discovered that DMs are also highly effective general problem solvers. Specifically, we focus on structured geometry modeling (e.g., CAD models). We have recently made significant strides in vector graphicsfloorplan generation, HD map reconstruction, and spatial arrangement estimation. DM-based approaches consistently achieve the best performance across all tasks, surpassing existing state-of-the-art methods tailored to specific tasks.</p>
        
        
            
        
    </div>
</div>
<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Self-Supervised 3D Scene Understanding</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src="&#x2F;vincent_lepetit.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>Vincent Lepetit</b>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">Professor, ENPC ParisTech, France</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
        Vincent Lepetit is a professor at ENPC ParisTech, France. Prior to this position, he was a full professor at the Institute for Computer Graphics and Vision, Graz University of Technology (TU Graz), Austria and before that, a senior researcher at CVLab, Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland.  His current research focuses on 3D scene understanding, especially at trying to reduce the supervision needed by a system to learn new 3D objects and new 3D environments. In 2020, he received with colleagues the Koenderick â€œtest-of-timeâ€ award for â€œBrief: Binary Robust Independent Elementary Featuresâ€. He often serves as an area chair of major computer vision conferences (CVPR, ICCV, ECCV) and as an editor for the Pattern Analysis and Machine Intelligence (PAMI) and International Journal of Computer Vision (IJCV) journals.  He was awarded in 2023 an ERC Advanced Grant for the 'explorer' project on creating digital twins of large-scale sites.
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š8æœˆ2æ—¥ï¼ˆæ°´ï¼‰ 
         13:30ã€œ14:30 
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶7å·é¤¨æƒ…å ±2è¬›ç¾©å®¤ï¼ˆ1éš 101ï¼‰
        </p>
        
        
        <p class="card-text">I will present our recent work on how a general AI algorithm can be used for 3D scene understanding to reduce the need for training data. More exactly, we propose several modifications of the Monte Carlo Tree Search (MCTS) algorithm to retrieve objects and room layouts from noisy RGB-D scans.  While MCTS was developed as a game-playing algorithm, we show it can also be used for complex perception problems.  Our adapted MCTS algorithm has few easy-to-tune hyperparameters and can optimise general losses. We use it to optimise the posterior probability of objects and room layout hypotheses given the RGB-D data.  This results in a render-and-compare method that explores the solution space efficiently. I will then show that the same algorithm can be applied to other scene understanding problems.</p>
        
        
            
        
    </div>
</div>
<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Making the Invisible Visible: Toward High-Quality Deep THz Computational Imaging</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src="&#x2F;chiawenlin.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>Chia-Wen Lin</b>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">Professor, National Tsing Hua University, Taiwan</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
        Prof. Chia-Wen Lin is currently a Professor with the Department of Electrical Engineering, National Tsing Hua University (NTHU), Taiwan. He also serves as Deputy Director of the AI Research Center of NTHU. He is currently a Visiting Professor at the Graduate School of Informatics, Informatics, Kyoto University from July 2023 to December 2023. He served as Visiting Professor at Nagoya University and National Institute of Informatics, Japan, in 2019 and 2015, respectively. His research interests include image/video processing, computer vision, and video networking. Dr. Lin is an IEEE Fellow, and has been serving on IEEE Circuits and Systems Society (CASS) Fellow Evaluating Committee since 2021. He serves as IEEE CASS BoG member-at-Large during 2022-2024. He was Steering Committee Chair of IEEE ICME (2020-2021), IEEE CASS Distinguished Lecturer (2018-2019), and President of the Chinese Image Processing and Pattern Recognition (IPPR) Association, Taiwan (2019-2020). He has served as Associate Editor of IEEE Transactions on Image Processing, IEEE Transactions on Multimedia, IEEE Transactions on Circuits and Systems for Video Technology, and IEEE Multimedia. He served as TPC Chair of IEEE ICME in 2010 and IEEE ICIP in 2019, and the Conference Chair of IEEE VCIP in 2018.
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š7æœˆ7æ—¥ï¼ˆé‡‘ï¼‰ 
         13:30ã€œ14:30 
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶7å·é¤¨æƒ…å ±2è¬›ç¾©å®¤ï¼ˆ1éš 101ï¼‰
        </p>
        
        
        <p class="card-text">Terahertz (THz) computational imaging has recently attracted significant attention thanks to its non-invasive, non-destructive, non-ionizing, material-classification, and ultra-fast nature for 3D object exploration and inspection. However, its strong water absorption nature and low noise tolerance lead to undesired blurs and distortions of reconstructed THz images. The performances of existing methods are highly constrained by the diffraction-limited THz signals. In this talk, we will introduce the characteristics of THz imaging and its applications. We will also show how to break the limitations of THz imaging with the aid of complementary information between the THz amplitude and phase images sampled at prominent frequencies (i.e., the water absorption profile of THz signal) for THz image restoration. To this end, we propose a novel physics-guided deep neural network design, namely Subspace-Attention-guided Restoration Network (SARNet), that fuses such multi-spectral features of THz images for effective restoration. Furthermore, we experimentally construct an ultra-fast THz time-domain spectroscopy system covering a broad frequency range from 0.1 THz to 4 THz for building up temporal/spectral/spatial/phase/material THz database of hidden 3D objects.</p>
        
        
            
        
    </div>
</div>
<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Toward Real-World Photometric Stereo: Why Photometric Stereo Must Be Universal?</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src="&#x2F;ikehata.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>æ± ç•‘ è«­</b>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ åŠ©æ•™</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
        He received a B.A. in Psychology in 2009, and an M.S. and Ph.D. in Information Studies in 2011 and 2014, respectively, from the University of Tokyo. He worked as a postdoctoral researcher at Washington University in St. Louis from 2014 to 2016. Currently, he is an assistant professor at the National Institute of Informatics, a specially appointed associate professor at Tokyo Tech, and a visiting researcher at the University of Tokyo. His main interests lie in 3D computer vision, with a particular focus on the use of photometric stereo to achieve professional-grade 3D reconstruction in real-world scenarios.
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š6æœˆ2æ—¥ï¼ˆé‡‘ï¼‰ 
         13:30ã€œ14:30 
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶7å·é¤¨æƒ…å ±2è¬›ç¾©å®¤ï¼ˆ1éš 101ï¼‰
        </p>
        
        
        <p class="card-text">Photometric stereo is a longstanding computer vision problem focused on recovering a detailed surface normal map of objects in a scene using images captured under varying illumination. Despite the simplicity of the basic problem statement, the underlying problem formulation and image acquisition setups are incredibly complex. To apply a photometric stereo method, the appropriate algorithm must be selected and images carefully captured in a controlled environment, taking into consideration assumptions about surface geometry, material, camera, and lighting. In this talk, I share my journey toward overcoming the fundamental challenges in photometric stereo by introducing the learning-based photometric stereo method, named "universal" photometric stereo, which aims to remove these complicated assumptions and acquisition setups. When performing deep learning on photometric stereo tasks, managing varying numbers of unordered input images under different lighting conditions is a critical challenge. I will firstly discuss my previous attempts to address this issue, including the development of an "observation map" (ECCV 2018, ICIP 2021) and a "light-axis transformer" (BMVC 2021), and how these ideas were extended to universal photometric stereo networks.<br><br>The core of my talk presents my recent universal photometric stereo networks (UniPS, CVPR 2022 and SDM-UniPS, CVPR 2023), which can recover impressively detailed surface normal maps even when images are captured under unknown, spatially-varying lighting conditions in uncontrolled environments. This advancement enables the application of photometric stereo in everyday settings, effectively allowing for Real-World Photometric Stereo.</p>
        
        
            
        
    </div>
</div>
<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Spatial AIã¨ã‚·ãƒ¼ãƒ³ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src="&#x2F;sakurada.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>æ«»ç”° å¥</b>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">ç”£æ¥­æŠ€è¡“ç·åˆç ”ç©¶æ‰€ ä¸»ä»»ç ”ç©¶å“¡</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
        2015å¹´æ±åŒ—å¤§å­¦å¤§å­¦é™¢æƒ…å ±ç§‘å­¦ç ”ç©¶ç§‘åšå£«å¾ŒæœŸèª²ç¨‹ä¿®äº†ã€‚æ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šç‰¹åˆ¥ç ”ç©¶å“¡(DC2)ã€ã‚«ãƒ¼ãƒã‚®ãƒ¼ãƒ¡ãƒ­ãƒ³å¤§å­¦å®¢å“¡ç ”ç©¶å“¡ã€æ±äº¬å·¥æ¥­å¤§å­¦åšå£«ç ”ç©¶å“¡ã€åå¤å±‹å¤§å­¦åŠ©æ•™ã€ç”£æ¥­æŠ€è¡“ç·åˆç ”ç©¶æ‰€å®¢å“¡ç ”ç©¶å“¡ã‚’çµŒã¦ã€2018å¹´4æœˆã‚ˆã‚Šç”£æ¥­æŠ€è¡“ç·åˆç ”ç©¶æ‰€ã«ä¸»ä»»ç ”ç©¶å“¡ã¨ã—ã¦å‹¤å‹™ã€‚
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š5æœˆ12æ—¥ï¼ˆé‡‘ï¼‰ 
         13:30ã€œ14:30 
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶7å·é¤¨æƒ…å ±2è¬›ç¾©å®¤ï¼ˆ1éš 101ï¼‰
        </p>
        
        
        <p class="card-text">è‡ªå‹•é‹è»¢è»Šã‚„ãƒ‰ãƒ­ãƒ¼ãƒ³ã€ã‚µãƒ¼ãƒ“ã‚¹ãƒ­ãƒœãƒƒãƒˆã€ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ãªã©ã€ã‚«ãƒ¡ãƒ©ã‚’æ­è¼‰ã—ãŸç§»å‹•ä½“ãŒæ™®åŠã™ã‚‹ã«ã¤ã‚Œã€ï¼“æ¬¡å…ƒãƒãƒƒãƒ—ã®é‡è¦æ€§ãŒä»¥å‰ã‚ˆã‚Šã‚‚å¤§å¹…ã«é«˜ã¾ã£ã¦ã„ã¾ã™ã€‚åŒæ™‚ã«ã€åºƒåŸŸã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒçš„å®¹æ˜“ã«åé›†ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã€ï¼“æ¬¡å…ƒãƒãƒƒãƒ—ã®æ§‹ç¯‰ã ã‘ã§ãªãã€å®Ÿä¸–ç•Œã®äº‹è±¡ã‚’è§£æã™ã‚‹ç ”ç©¶ã‚‚ç››ã‚“ã«è¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«ã€ãƒ¢ãƒã‚¤ãƒ«ã‚«ãƒ¡ãƒ©ã§åé›†ã—ãŸå‹•ç”»åƒã®ã‚·ãƒ¼ãƒ³ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã«ã‚‚æ³¨ç›®ãŒé›†ã¾ã£ã¦ã„ã¾ã™ã€‚æœ¬è¬›æ¼”ã§ã¯ã€ï¼“æ¬¡å…ƒãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€ã‚·ãƒ¼ãƒ³èªè­˜ã€ã‚·ãƒ¼ãƒ³ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ã‚’ä¸­å¿ƒã«ã€ç©ºé–“ãƒ¢ãƒ‡ãƒªãƒ³ã‚°æŠ€è¡“ã®æœ€æ–°å‹•å‘ã¨è‡ªèº«ã®å–ã‚Šçµ„ã¿ã«ã¤ã„ã¦è§£èª¬ã—ã¾ã™ã€‚</p>
        
        
            
        
    </div>
</div>
<p style="height: 24px">
<a href="/2024"><<2024å¹´ã®è¬›æ¼”ã¯ã“ã¡ã‚‰</a><a class="float-right" href="/2022">2022å¹´ã®è¬›æ¼”ã¯ã“ã¡ã‚‰>></a>
</p>


            </div>
        </section>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
    </body>
</html>
