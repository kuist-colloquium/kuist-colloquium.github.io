<!DOCTYPE html>
<html lang="ja">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <title>KUIST Colloquium 
2023
</title>
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
        <link rel="stylesheet" href="/style.css">
    </head>
    <body>
        <header style="background: #00205B;">
            <img src="/C-EL-W.svg" height=45 style="margin: 1em 0em 0em 1em;">
            <h1 style="color: #FFF; text-align: center; font-family: serif; font-size:3em; padding: 0.4em;">
                IST COLLOQUIUM 
2023

            </h1>
        </header>
        <section class="section">
            <div class="container">
                
<p><div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Spatial AIとシーンプライバシー</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src="sakurada.jpg" alt="講演者の画像" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;"><b>櫻田 健</b></h3>
        <h5 class="card-text" style="margin-bottom: 22px;">産業技術総合研究所 主任研究員</h5>
        
        <details style="margin-bottom: 22px">
        <summary>講演者経歴</summary>
        2015年東北大学大学院情報科学研究科博士後期課程修了。日本学術振興会特別研究員(DC2)、カーネギーメロン大学客員研究員、東京工業大学博士研究員、名古屋大学助教、産業技術総合研究所客員研究員を経て、2018年4月より産業技術総合研究所に主任研究員として勤務。
        </details>
        
        <p, class="card-text">
        <b>日時</b>：5月12日（金） 
         13:30時〜14:30時 
        </p>
        
        <p class="card-text">
        <b>場所</b>：総合研究7号館情報2講義室（1階 101）
        </p>
        
        
        <p class="card-text">自動運転車やドローン、サービスロボット、スマートフォンなど、カメラを搭載した移動体が普及するにつれ、３次元マップの重要性が以前よりも大幅に高まっています。同時に、広域のデータを比較的容易に収集できるようになり、３次元マップの構築だけでなく、実世界の事象を解析する研究も盛んに行われています。さらに、モバイルカメラで収集した動画像のシーンプライバシーにも注目が集まっています。本講演では、３次元モデリング、シーン認識、シーンプライバシー保護を中心に、空間モデリング技術の最新動向と自身の取り組みについて解説します。</p>
        
        
            
        
    </div>
</div>

<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Toward Real-World Photometric Stereo: Why Photometric Stereo Must Be Universal?</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src="ikehata.jpg" alt="講演者の画像" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;"><b>池畑 諭</b></h3>
        <h5 class="card-text" style="margin-bottom: 22px;">国立情報学研究所 助教</h5>
        
        <details style="margin-bottom: 22px">
        <summary>講演者経歴</summary>
        He received a B.A. in Psychology in 2009, and an M.S. and Ph.D. in Information Studies in 2011 and 2014, respectively, from the University of Tokyo. He worked as a postdoctoral researcher at Washington University in St. Louis from 2014 to 2016. Currently, he is an assistant professor at the National Institute of Informatics, a specially appointed associate professor at Tokyo Tech, and a visiting researcher at the University of Tokyo. His main interests lie in 3D computer vision, with a particular focus on the use of photometric stereo to achieve professional-grade 3D reconstruction in real-world scenarios.
        </details>
        
        <p, class="card-text">
        <b>日時</b>：6月2日（金） 
         13:30時〜14:30時 
        </p>
        
        <p class="card-text">
        <b>場所</b>：総合研究7号館情報2講義室（1階 101）
        </p>
        
        
        <p class="card-text">Photometric stereo is a longstanding computer vision problem focused on recovering a detailed surface normal map of objects in a scene using images captured under varying illumination. Despite the simplicity of the basic problem statement, the underlying problem formulation and image acquisition setups are incredibly complex. To apply a photometric stereo method, the appropriate algorithm must be selected and images carefully captured in a controlled environment, taking into consideration assumptions about surface geometry, material, camera, and lighting. In this talk, I share my journey toward overcoming the fundamental challenges in photometric stereo by introducing the learning-based photometric stereo method, named "universal" photometric stereo, which aims to remove these complicated assumptions and acquisition setups. When performing deep learning on photometric stereo tasks, managing varying numbers of unordered input images under different lighting conditions is a critical challenge. I will firstly discuss my previous attempts to address this issue, including the development of an "observation map" (ECCV 2018, ICIP 2021) and a "light-axis transformer" (BMVC 2021), and how these ideas were extended to universal photometric stereo networks.<br><br>The core of my talk presents my recent universal photometric stereo networks (UniPS, CVPR 2022 and SDM-UniPS, CVPR 2023), which can recover impressively detailed surface normal maps even when images are captured under unknown, spatially-varying lighting conditions in uncontrolled environments. This advancement enables the application of photometric stereo in everyday settings, effectively allowing for Real-World Photometric Stereo.</p>
        
        
            
        
    </div>
</div>
</p>
<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Making the Invisible Visible: Toward High-Quality Deep THz Computational Imaging</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src="chiawenlin.jpg" alt="講演者の画像" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;"><b>Chia-Wen Lin</b></h3>
        <h5 class="card-text" style="margin-bottom: 22px;">Professor, National Tsing Hua University, Taiwan</h5>
        
        <details style="margin-bottom: 22px">
        <summary>講演者経歴</summary>
        Prof. Chia-Wen Lin is currently a Professor with the Department of Electrical Engineering, National Tsing Hua University (NTHU), Taiwan. Japan. He also serves as Deputy Director of the AI Research Center of NTHU. He is currently a Visiting Professor at the Graduate School of Informatics, Informatics, Kyoto University from July 2023 to December 2023. He served as Visiting Professor at Nagoya University and National Institute of Informatics, Japan, in 2019 and 2015, respectively. His research interests include image&#x2F;video processing, computer vision, and video networking. Dr. Lin is an IEEE Fellow, and has been serving on IEEE Circuits and Systems Society (CASS) Fellow Evaluating Committee since 2021. He serves as IEEE CASS BoG member-at-Large during 2022-2024. He was Steering Committee Chair of IEEE ICME (2020-2021), IEEE CASS Distinguished Lecturer (2018-2019), and President of the Chinese Image Processing and Pattern Recognition (IPPR) Association, Taiwan (2019-2020). He has served as Associate Editor of IEEE Transactions on Image Processing, IEEE Transactions on Multimedia, IEEE Transactions on Circuits and Systems for Video Technology, and IEEE Multimedia. He served as TPC Chair of IEEE ICME in 2010 and IEEE ICIP in 2019, and the Conference Chair of IEEE VCIP in 2018.
        </details>
        
        <p, class="card-text">
        <b>日時</b>：7月7日（金） 
         13:30時〜14:30時 
        </p>
        
        <p class="card-text">
        <b>場所</b>：総合研究7号館情報2講義室（1階 101）
        </p>
        
        
        <p class="card-text">Terahertz (THz) computational imaging has recently attracted significant attention thanks to its non-invasive, non-destructive, non-ionizing, material-classification, and ultra-fast nature for 3D object exploration and inspection. However, its strong water absorption nature and low noise tolerance lead to undesired blurs and distortions of reconstructed THz images. The performances of existing methods are highly constrained by the diffraction-limited THz signals. In this talk, we will introduce the characteristics of THz imaging and its applications. We will also show how to break the limitations of THz imaging with the aid of complementary information between the THz amplitude and phase images sampled at prominent frequencies (i.e., the water absorption profile of THz signal) for THz image restoration. To this end, we propose a novel physics-guided deep neural network design, namely Subspace-Attention-guided Restoration Network (SARNet), that fuses such multi-spectral features of THz images for effective restoration. Furthermore, we experimentally construct an ultra-fast THz time-domain spectroscopy system covering a broad frequency range from 0.1 THz to 4 THz for building up temporal/spectral/spatial/phase/material THz database of hidden 3D objects.</p>
        
        
            
        
    </div>
</div>
<p><a class="float-right" href="2022">2022年の講演はこちら&gt;&gt;</a></p>


            </div>
        </section>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
    </body>
</html>
