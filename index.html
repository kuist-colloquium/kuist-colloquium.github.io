<!DOCTYPE html>
<html lang="ja">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <title>KUIST Colloquium 
2025
</title>
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
        <link rel="stylesheet" href="/style.css">
    </head>
    <body>
        <header style="background: #00205B;">
            <img src="/C-EL-W.svg" height=45 style="margin: 1em 0em 0em 1em;">
            <h1 style="color: #FFF; text-align: center; font-family: serif; font-size:3em; padding: 0.4em;">
                IST COLLOQUIUM 
2025

            </h1>
        </header>
        <section class="section">
            <div class="container">

<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>We Are (Still!) Not Giving Data Enough Credit in Visual Computing</b></h2>

        <div style="float:right; width: max(25%, 130px);">
        <img src=".&#x2F;alexei.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>       
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>Alexei (Alyosha) Efros</b>
        
            <span style="font-size: 1.25rem"><a href="https://people.eecs.berkeley.edu/~efros/">ğŸŒ</a></span>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">Howard Friesen Professor, EECS Department, UC Berkeley, USA</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
            Alexei (Alyosha) Efros joined UC Berkeley in 2013.
            Prior to that, he was for a decade on the faculty of Carnegie Mellon University, and has also been affiliated with Ã‰cole Normale SupÃ©rieure/INRIA and University of Oxford.
            His research is in the area of computer vision and computer graphics, especially at the intersection of the two.
            He is particularly interested in using data-driven techniques to tackle problems where large quantities of unlabeled visual data are readily available.
            Efros received his PhD in 2003 from UC Berkeley.
            He is a recipient of CVPR Best Paper Award (2006), Sloan Fellowship (2008), Guggenheim Fellowship (2008), Okawa Grant (2008), SIGGRAPH Significant New Researcher Award (2010), three PAMI Helmholtz Test-of-Time Prizes (1999,2003,2005), the ACM Prize in Computing (2016), Diane McEntyre Award for Excellence in Teaching Computer Science (2019), Jim and Donna Gray Award for Excellence in Undergraduate Teaching of Computer Science (2023), and PAMI Thomas S. Huang Memorial Prize (2023).
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š 1æœˆ21æ—¥ï¼ˆæ°´ï¼‰ 
            13:15-14:45
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶ï¼—å·é¤¨è¬›ç¾©å®¤1ï¼ˆ1éšã€€107)
        </p>
        
        <p class="card-text">
            For most of its existence, Visual Computing has been primarily focused on algorithms, with data treated largely as an afterthought.
            Only recently, with the advances in AI, did our field start to truly appreciate the singularly crucial role played by data, but even now we might still be underestimating it.
            In this talk, I will begin with some historical examples illustrating the importance of large visual data for visual analysis and synthesis.
            I will then share some of our recent work demonstrating the power of very simple algorithms when used with the right data, including scene analysis, model interpolation, and visual data attribution.
        </p>
    </div>
</div>



<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Can a Camera be Self-Sustaining?</b></h2>

        <div style="float:right; width: max(25%, 130px);">
        <img src=".&#x2F;shree.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>       
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>T.C. Chang Prof. Shree K. Nayar</b>
        
            <span style="font-size: 1.25rem"><a href="https://www.cs.columbia.edu/~nayar/">ğŸŒ</a></span>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">Dept of Computer Science, Columbia University, USA</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
            Shree K. Nayar is the T. C. Chang Professor of Computer Science at Columbia University.
            He heads the Columbia Imaging and Vision Laboratory (CAVE), which develops computational imaging and computer vision systems.
            Nayar received his PhD degree in Electrical and Computer Engineering from the Robotics Institute at Carnegie Mellon University.
            For his research and teaching he has received several honors including the David Marr Prize (1990 and 1995), the David and Lucile Packard Fellowship (1992), the National Young Investigator Award (1993), the NTT Distinguished Scientific Achievement Award (1994), the Keck Foundation Award for Excellence in Teaching (1995), the Columbia Great Teacher Award (2006), the Carnegie Mellon Alumni Achievement Award (2009), Sony Appreciation Honor (2014), the Columbia Engineering Distinguished Faculty Teaching Award (2015), the IEEE PAMI Distinguished Researcher Award (2019), the Funai Achievement Award (2021), and the Okawa Prize (2022).
            For his contributions to computer vision and computational imaging, he was elected to the National Academy of Engineering in 2008, the American Academy of Arts and Sciences in 2011, and the National Academy of Inventors in 2014.
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š 12æœˆ4æ—¥ï¼ˆæœ¨ï¼‰ 
            15:00-16:00
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶ï¼—å·é¤¨è¬›ç¾©å®¤1ï¼ˆ1éšã€€107)
        </p>
        
        <p class="card-text">
            For cameras to be more widely deployed, there are two important requirements they need to satisfy.
            First, they need to be self-sustaining, in that, they should be able to capture and transmit visual information without the use of a power supply and without being tethered to other devices or infrastructure.
            Second, when used in applications that do not require identification of humans, they should preserve privacy. <br>
            We will present recent results on the creation of such self-sustaining imaging systems.
            Our approach uses energy harvested from the light falling on the camera to fully power the camera, enabling it to make visual measurements as well as wirelessly transmit these measurements, without the use of a battery or an external power supply.
            We will conclude the talk with a related result that enables forecasting the energy harvested by a solar panel from a single image taken from close to it.
        </p>
    </div>
</div>


<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>The Ultimate Video Camera</b></h2>

        <div style="float:right; width: max(25%, 130px);">
        <img src=".&#x2F;kyros-1.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>       
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>Prof. Kyros Kutulakos</b>
        
            <span style="font-size: 1.25rem"><a href="https://www.cs.toronto.edu/~kyros/">ğŸŒ</a></span>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">Dept. of Computer Science, University of Toronto, Canada</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
            Kyros is a Professor of Computer Science at the University of Toronto and an expert in computational imaging and computer vision.
            His research over the past decade has focused on combining programmable light sources, sensors, optics and algorithms to create cameras with unique capabilities&mdash;from seeing through scatter and looking around corners to capturing surfaces with complex material properties robustly in 3D.
            He is currently leading efforts to harness the potential of technologies such as single-photon cameras and programmable-pixel image sensors for applications in extreme computer vision and scientific imaging.
            Kyros is a recipient of an Alfred P. Sloan Fellowship, an NSF CAREER Award, and eight paper awards at ICCV, CVPR and ECCV, including two Marr Prizes (2023 and 1999), and the CVPR Best Paper Award in 2019.
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š11æœˆ28æ—¥ï¼ˆé‡‘ï¼‰ 
            13:15-14:45
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶ï¼—å·é¤¨ã‚»ãƒŸãƒŠãƒ¼å®¤2ï¼ˆï¼‘éšã€€131)
        </p>
        
        <p class="card-text">
            Over the past decade, advances in image sensor technologies have transformed the 2D and 3D imaging capabilities of our smartphones, cars, robots, drones, and scientific instruments.
            As these technologies continue to evolve, what new capabilities might they unlock?
            I will discuss one possible point of convergence&mdash;the ultimate video camera&mdash;which is enabled by emerging single-photon image sensors and photon-processing algorithms.
            We will explore the extreme imaging capabilities of this camera within the broader historical context of high-speed imaging systems, highlighting its potential to capture the physical world in entirely new ways.
        </p>
        
        
            
        
    </div>
</div>


<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Advancing Visual Understanding and Generation: Recent Research from NTHU Computer Vision Lab</b></h2>

        <div style="float:right; width: max(25%, 130px);">
        <img src=".&#x2F;shlai.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>       
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>Prof. Shang-Hong Lai</b>
        
            <span style="font-size: 1.25rem"><a href="https://www.cs.nthu.edu.tw/~lai/">ğŸŒ</a></span>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">Dept. of Computer Science, National Tsing Hua University, Taiwan</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
            Shang-Hong Lai is a Professor in the Department of Computer Science at National Tsing Hua University (NTHU), Taiwan, where he also serves as the Associate Dean of the College of Electrical Engineering and Computer Science (EECS). From 2018 to 2022, Dr. Lai was on leave at the Microsoft AI R&D Center in Taiwan, where he worked as a Principal Research Manager, leading a science team focused on face-related AI research.
            Dr. Laiâ€™s research interests span computer vision, image processing, and machine learning. He has authored over 300 publications in leading international journals and conferences in these fields and holds approximately 30 patents related to computer vision and medical imaging technologies. He has served as Area, Theme, or Program Chair for major international conferences such as CVPR, ICCV, ECCV, NeurIPS, ICML, IJCAI, ACCV, and ICPR, and as an Associate Editor for International Journal of Computer Vision (IJCV), IEEE Transactions on Image Processing (TIP), and Pattern Recognition.
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š11æœˆ18æ—¥ï¼ˆç«ï¼‰ 
            10:30-12:00
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶7å·é¤¨ã‚»ãƒŸãƒŠãƒ¼å®¤1ï¼ˆ1éš 127ï¼‰
        </p>
        
        
        <p class="card-text">
            In this talk, I will present some recent research works from the Computer Vision Laboratory at National Tsing Hua University (NTHU). The NTHU CV Lab focuses on several key areas, including video understanding, face-related analysis, anomaly detection, and medical imaging.
            I will begin with two of our latest advances in video understanding, HERMES and VADER. HERMES introduces two versatile modules that can be seamlessly integrated into existing videoâ€“language models or deployed as a standalone framework for long-form video comprehension, achieving state-of-the-art performance across multiple benchmarks. VADER, on the other hand, is an LLM-driven framework for video anomaly reasoning, which combines keyframe-level object-relation modeling with visual contextual cues to enhance anomaly interpretation.
            Next, I will discuss one of our recent works in anomaly detection, LFQUIAD. LFQUIAD integrates a quantization-driven autoencoder with a modular Anomaly Generation Module to improve representation learning. Finally, I will briefly present two medical imaging projects leveraging diffusion modelsâ€”one for generating paired 3D CT imageâ€“mask datasets, and the other for synthesizing contrast-enhanced 3D CT volumes from non-contrast scans.
            Through these examples, I will highlight our labâ€™s ongoing efforts toward building generalizable, interpretable, and efficient computer vision systems bridging visual understanding and generative modeling.
        </p>
        
        
            
        
    </div>
</div>


<div class="card" style="width: min(90%, 1200); margin: 50px; margin-left: auto; margin-right: auto; display: block;">
    <div class="card-body" style="overflow:hidden">
        
        <h2 style="border-bottom: solid; margin-bottom:20px; padding-bottom:2px; border-color:#00205B;"><b>Vision at Work: Reflections on Real-World Computer Vision from Robots to Video Understanding and Back</b></h2>
        
        
        <div style="float:right; width: max(25%, 130px);">
        <img src=".&#x2F;austin_myers_profile_pic.jpg" alt="è¬›æ¼”è€…ã®ç”»åƒ" style="width:100%; padding: 0px 0px 10px 10px;">
        </div>
        
        <h3 class="card-title" style="margin-bottom: 6px;">
            <b>Austin Myers</b>
        
            <span style="font-size: 1.25rem"><a href="https://austinomyers.com/">ğŸŒ</a></span>
        
        </h3>
        <h5 class="card-text" style="margin-bottom: 22px;">Agility Robotics</h5>
        
        <details style="margin-bottom: 22px">
        <summary>è¬›æ¼”è€…çµŒæ­´</summary>
        Austin Myers is a research engineer currently working on embodied perception at Agility Robotics. His career has spanned academia and industry, with roles at Waymo, Google Research, and DeepMind, where he developed vision systems for robot manipulation, self-driving vehicles, and large-scale video understanding. Austin received his PhD from the University of Maryland, focusing on understanding the affordances of object parts through geometric reasoning. His broad research interests lie at the intersection of joint video and language representation learning, large multimodal models, and embodied perception for everyday robots.
        </details>
        
        <p, class="card-text">
        <b>æ—¥æ™‚</b>ï¼š7æœˆ7æ—¥ï¼ˆæœˆï¼‰ 
         13:15ã€œ14:45 
        </p>
        
        <p class="card-text">
        <b>å ´æ‰€</b>ï¼šç·åˆç ”ç©¶7å·é¤¨1éšæƒ…å ±2è¬›ç¾©å®¤
        </p>
        
        
        <p class="card-text">Over the past decade, Iâ€™ve worked on perception systems spanning everyday robot manipulation, self-driving cars, and large-scale video understanding across academia and industry labs like Waymo, Google Research, DeepMind, and now Agility Robotics. In this talk, Iâ€™ll share perspectives on developments in the field and industry, and lessons from deploying computer vision systems in the real world. Iâ€™ll tell you why construction cones are harder than they look, what it takes to build vision-language models that understand YouTube videos without human supervision, how to transfer fundamental research to products that touch billions of users, and Iâ€™ll revisit how we once tried to enable robots to use tools they had never seen beforeâ€”and how we've come back full circle with new tools at our disposal in my work at Agility Robotics. Beyond the research itself, Iâ€™ll reflect on navigating careers between academia and industry, choosing impactful problems, and the exciting challenges ahead in embodied AI.</p>
        
        
            
        
    </div>
</div>
<p style="height: 24px">
<a class="float-right" href="/2024">2024å¹´ã®è¬›æ¼”ã¯ã“ã¡ã‚‰>></a>
</p>


            </div>
        </section>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
    </body>
</html>
